---
title: "Statistics without spreadsheet"
author: "Tomasz Przechlewski"
division: "Powiślańska Szkoła Wyższa (Kwidzyn/Poland)"
description: (c) Tomasz Przechlewski / CC-BY license 
date: "March, 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

## Who am I

My name is Tomasz Plata-Przechlewski and I live in Poland. I was born on 16th june 1963
(it was Sunday, the exact day when Valentina 
Vladimirovna Tereshkova was launched into space 
-- if you know who she is).

BTW in Poland born-in-sunday means work-shy (ie. lazy) person (so you know now first Polish? proverb)

BTW by pure statistics $1/7 \approx 14$% of the population is work-shy:-)

I graduated economy long time ago and taught statistics and information systems (mainly).
I am a big fan of open source software (or OSS) and I knew a few OSS systems including Linux and LaTeX.
And of course R which I am about to show you in a while.

![](./TPphoto.jpg)

My hobby is Road Cycling and History. A I am also a amateur photographer.
(cf [tprzechlewski@flickr](https://www.flickr.com/photos/tprzechlewski/))

## Agenda

Statistics (nothing spectacular, just classical EDA, no (heavy) math, relax)

Statistical software (modern, non-standard or hipster #youcall)

Poland (via statistical examples)

## How statistics is taught to students at social-science departments (at least in Poland)

Three components of Statistics:

Theory (models) + Tools (programs) + Practice (real data)

Undergraduate courses in social sciences in Poland
concentrate on theory, use Spreadsheet as an universal computing tool
Office-like editor (MS Word/OO Writes)
as an universal publishing tool. Students works with artificial (clean)
and small data sets thus are unaware of problems related to
applying theory to practice. 
The workflow is shown in the following diagram:

![](./SAnalasis3.png)


It is claimed that the above scenario is the best one. More advanced
tools would be too difficult (and time consuming) to be acquainted
to by students, thus distracting them
from the main subject of the course, ie statistical methods. 

## How statistics should be taught to students (in my opinion at least)

Office software has limits. Spreadsheets are good for number crunching,
but are not so good in: data cleaning (Practice), advanced graphics,
spatial analysis (Practical-Theory), team work (Practice).

Office editors or Powerpoint/ are great tools
but are not quality publishing of statistical results.

It is wrong to ignore the existence of modern open source tools and not
introduce students to them. It is wrong not to introduce students
to some (even elementary) programming,
and sticking exclusively to
*point-and-click* mode of work (ie spreadsheet).

I will try to demonstrate that using modern tools for statistical
analysis is a feasible way to go. That (some) modern tools are not much more
(prohibitive) difficult that office software (at higher than basic level)

Conclusion: less theory, more practice and common sense. Show student
real *'value chain' of statistical analysis with all its problems (not covered
nowadays): 

* *Poor definitions*: 
complexity of statistical data collection: imprecise, complex, *unintuitive*,
contradictory etc definitions, 
imprecise (or worse meaningless) measurement, 
unreliable (incomplete, erroneous) data etc...

* *Modern tools*: programmable, better quality (graphics), more reliable

* *New workflow* based on reproducible research concept (expained momentarily)

![](./SAnalasis1.png)

## Learnig curve comparison

Learning curve of programmable vs point-and-click (direct-manipulation) tools:

![](./R_excel_LC.png)

For simple task learning programmable tools is waste of time but
as task complexity increases it is more beneficial to use same programmable tool
(cf [Scientific computing: Code alert](https://www.nature.com/nature/journal/v541/n7638/full/nj7638-563a.html)) 

## Poor definitions example #1: Full Time Equivalence (FTE)

Number of students.

**Who is a student?**

Student is a person attending to
a 3rd level status school in in the
3-stage education system (cf [Educational_stage](https://en.wikipedia.org/wiki/Educational_stage)).
The answer is still non-obvious as there are many
forms of tertiary education. For example:

The UNESCO stated that tertiary education focuses on learning
endeavors in specialized fields. It includes academic and higher
vocational education.

So according to the above definition the school do not belongs
to tertiary education if its status is not academic and/or higher
vocational. Example:  Dance Academy or
University for Elderly people (aka University of the 3rd Age).
Both are popular in Poland.

In many countries there are some certification scheme. For example
in Poland a school must apply (and get) a certificate to be
regarded as high school (ie part of tertiary level of education)

**Heads vs Majors**

Student can be enrolled to more than one course (major).
So for counting heads it is necessary  to remove duplicates
otherwise one would count majors not persons.

**Part time studies**

FTE stands for Full-Time-Equivalent, an approximation of the
number of students who would be enrolled full-time

Full time equivalent (FTE) -- FTE is based on student credit
hours. It is obtained by dividing student credit
hours by some a number of credit hours for full-time-study.

Conclusion: Majors, Persons or FTEs? Which is the best?

[University of Utah/Office of Analysis,
Assessment and Accreditation](http://www.usu.edu/aaa/about_our_data.cfm)
google:single multiple majors fte

## Poor definitions example #2: measurement of tourism activity [concept of an Indicator]

**Who is a tourist**. According to
[Glossary:Tourism](https://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:Tourism)

Tourism means the activity of visitors taking a trip
to a main destination outside their usual environment,
for less than a year, for any main purpose, including business,
leisure or other personal purpose,
other than to be employed by a resident entity in the place visited.

According to the above definition to be regarded as tourist one has
to change her/his accommodation place for less than
one year (otherwise Eurostat would regard her/him as migrant)

The usual meaning (at least in Poland) is that tourist
is travelling for leisure not to work. People travelling to work
has other needs/aims than those travelling to rest
(they usually do not use hotels for example)
so the above definition solves some problems but at the same time
creates many others.

Number of tourists: do not distinguish between various form of tourists,
difficult to collect (who is a tourist anyway?)

Various `number of' tourist-oriented establishments
(hotels, catering units, beds,
nights spent) etc. They do not measure tourists per-se but
are highly related and more reliable (as easier to count).

Indicator of tourist activity (by various tourist types).

Conclusion: measurement of tourism activity is not trivial
Other similar: internet user, migrant, unemployed person,
illiterate person

## Sloppy measurement: measurement of tourism activity [data collection]

Tourism supply statistics (accommodation statistics): Data on rented accommodation ie. capacity and occupancy of tourist accommodation
establishments in the reporting country. How collected? Registers?

**How statistical data is collected?**: exhaustive data vs sample. **Exhaustive**:
dedicated sureys (obligatory reports) vs administrative 
registers (births, deaths, police statistics). Sample: representative sample
vs random sample vs panel data. Panels (cf [Panel Research](https://www.questionpro.com/blog/panel-research-why-it-matters/)). Panels are overused nowadys (cf https://panelariadna.pl/):

Quirks of data collection:
Data up to year 2015 inclusive refer to only those units that made the statistical reports.
Starting of data for January 2016,
the method of imputation data was implemented (ie replacing
missing data with some (possibly meaningful :-)) values.
(cf [BDL](https://bdl.stat.gov.pl/BDL/dane/podgrup/temat/18/240/2396))

**Tourism demand statistics**: Data on participation in tourism of the residents of the reporting country.
How collected? Surveys?

Most of the time, data on domestic and outbound trips (where "outbound tourism" means residents of a country travelling in another country) is collected via sample
surveys (cf [Annual data on trips of EU residents](https://ec.europa.eu/eurostat/cache/metadata/en/tour_dem_esms.htm) and
[Tourism_statistics_-_top_destinations](https://ec.europa.eu/eurostat/statistics-explained/index.php/Tourism_statistics_-_top_destinations))

Regulations concerning data collection in tourism (hundreds of pages):
[Glossary:Supply_side_tourism_statistics](https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Glossary:Supply_side_tourism_statistics)
and [EU regulation No 692/2011](https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32011R0692)

So now we know what we are dealing with...

## Sloppy measurement: Example nights spent (demand side)

Share of nights spent at EU-28 tourist accommodation by tourists
travelling outside their own country of residence, 2017 
[Share of nights spent at EU-28 tourist accommodation by tourists
travelling outside their own country](http://appsso.eurostat.ec.europa.eu/nui/submitViewTableAction.do)

Country of residence = Foreign country (estimated data)

| year | 2008    | 2009    |    2010  |     2011 | 2012     | 2013     |     2014 |     2015 |  2016    | 2017  |
| -----|:-------:| -------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|------:|
| #    |10173237 | 9609447 | 10064628 | 10620264 | 11876599 | 12471268 | 12992241 | 13757657 | 15579225 | 16705215 |

```{r echo=FALSE, message=F, warning=F}
library(ggplot2)
library(ggpubr)
# line plot with trend (for x as.Date)
# center title

d <- read.csv("nights_spent_byc.csv", sep = ';',  header=T, na.string="NA");
yr <- as.Date(paste(d$year, "-01-01", sep=""));

d["yr"] <- yr

p1 <- ggplot(d, aes(x = yr)) +
 geom_line(aes(y = poland, colour = "poland"), size=2) +
 ylab(label="beds (number)") +
 labs(colour = "") +
 theme(legend.position="top") +
 ggtitle("nights spent by tourists travelling outside") +
 theme(plot.title = element_text(hjust = 0.5)) +
 theme(legend.text=element_text(size=12));

# index base = firstObs
pl <- diff(d$poland, differences = 1)

d$poland

pl <- append(NA, pl);

d["pl"] <- pl/d$poland * 100

p2 <- ggplot(d, aes(x = yr)) +
 geom_line(aes(y = pl, colour = "pl"), size=2) +
 ylab(label="beds (% change)") +
 labs(colour = "") +
 theme(legend.position="top") +
 ggtitle("nights spent by tourists travelling outside") +
 theme(plot.title = element_text(hjust = 0.5)) +
 theme(legend.text=element_text(size=12));

ggarrange(p1, p2, ncol = 2, nrow = 1)
```

to be continued...


## Poor definitions (final) example #3: dreadful aggregates

Indicators can be divided to hard indicators and soft indicators.
*Hard indicators* denote hard facts while soft indicators are 
*beliefs* and *intentions*. For example number of hotels is a fact,
while intention to stay abroad less than a year is not a fact but an intention.
In Poland at least 80% respondents declares they intend to vote, while
the true turnover never exceed 55%. In other words measuring something using soft
indicators is prone to (significant) errors.
 
 That not means that hard indicator is error-free. By definition it measures not
 the phenomenon but some proxy associated with the phenomenon. 
 
 With hard indicator we have precise measurement of imprecise measure. With
 soft indicator we have imprecise measurement of imprecise measure.
 
 To cure (or hide) the problems aggregates of indicators are constructed,
 eiter as sums (*indexes* or *formative*) or as averages (factors or *reflective*. 
 Indexes are more popular in economics
 while averages/factors are more popular in psychology, sociology etc...
 
 For example Gross National Product (GDP) is an index
 while (customer) satisfaction defined as some set of opinions on a product would be a factor.
 
 Control question: what is measured with GDP?
 
 Collection methods from most to last reliable:

* administrative registers (almost all)

* some obligatory reports, some sample-based

* panels, obligatory reports when respondent is unable/unwill/ to provide information or take part at all), most sample-based (intensions, soft factors)

Typical collection method description of a sample based survey:
Data collected from 1st April to 2nd April 2019.
Cross national panel (or sample). Respondent age +18. Panel size 1020. 
Quotas representative for sex, age and residence type

No information provided on non-response rate/non-contact rate (why?).


**Example**: How to measure illiteracy? A. Ask a straight question (can you read/write). B. Ask a question how many books respondent read last year, if zero = illiterate (nasty!).
C. Ask for certificate (infeasable). I wonder about illiteracy rate of many
countries if approach B would be excercised:-)

## Interlude: R and Rstudio

R is both *programming language*  for statistical computing and graphics and 
a software (ie application) to execute programs written in R.
R was developed in mid 90s  at the University of Auckland 
(New Zealand).

Since then R has become one of the dominant software environments for data analysis and is used by a variety of scientific disiplines.

BTW why it is called so strange (R)? Long time ago it was popular to use short names for computer languages
(C for example). At AT&T Bell Labs (John Chambers) in mid 70s a language oriented towards statistical
computing was developed and called S (from Statistics). R is one letter before S in an alphabet.

Rstudio is an *environment* through which to use R. In Rstudio one can simultaneously write code, execute
code it, manage data, get help, view plots. Rstudio is a commercial product distributed
under dual-license system by RStudio, Inc. Key developer of RStudio is Hadley Wickham 
another brilliant New Zealander (cf [Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham) )

Microsoft invest heavily into R development recently. It bought Revolution Analytics a key 
developer of R and provider of commercial versions of the system. With MS support the system
is expected to gain more popularity (for example 
by integrating it with popular MS products)

## Interlude: Measures of central tendency, dispersion and skewness with R
(univariate analysis)

The CSV file `hotele_caloroczne_PL.csv` contains data on number of all-season hotels 
in every county in Poland. First one has to load the dataset with the read.csv command:

```{r, pl_hotel_data, cache=T}
d <- read.csv("hotele_caloroczne_PL.csv", sep = ';',  header=T, na.string="NA")
```

Computing measures of central tendency (with summary and/or fivenum)

```{r, cache = F, dependson = "pl_hotel_data"}
summary(d)
fivenum(d$hotele2017)
```

Computing mean:

```{r, cache = F, dependson = "pl_hotel_data"}
mean(d$hotele2017)
```

And dispersion:

```{r, cache = F, dependson = "pl_hotel_data"}
var(d$hotele2012); var(d$hotele2017)
sd(d$hotele2012); sd(d$hotele2017)
```

Second attempt (with no output/respective values was saved as **variables** `var12`...`sd17`):

```{r, cache = F, dependson = "pl_hotel_data"}
var12 <- var(d$hotele2012, na.rm=T); var17 <- var(d$hotele2017, na.rm=T)
sd12 <- sd(d$hotele2012, na.rm=T); sd17 <- sd(d$hotele2017, na.rm=T);
```

BTW:

```{r, cache = F, dependson = "pl_hotel_data"}
c( mean(d$hotele2012, na.rm=T), mean(d$hotele2017, na.rm=T))
```

Or more formally. There were `r mean(d$hotele2012, na.rm=T)`
hotels on the average in every county in Poland in 2012 while
in 2017 there were `r mean(d$hotele2017, na.rm=T)` hotels.

Interquartile Range aka IQR which is 
the range from the upper (75%) quartile to the lower (25%) quartile. IQR represents 
central 50% observations of a population. 
IQR is a robust measure of dispersion, unaffected by the distribution of data:

```{r, cache = F, dependson = "pl_hotel_data"}
c( IQR(d$hotele2012, na.rm=T), IQR(d$hotele2017, na.rm=T))
```

Finally we can equally easily assess the skewenss:

```{r, cache = F, dependson = "pl_hotel_data"}
library(moments)
c(skewness(d$hotele2012, na.rm=T), skewness(d$hotele2017))
```

Distribution skewness is significant in both periods. 
Using (modified) Persons' formula $(\bar x -D )/ \sigma^2$ we obtain:
```{r, cache=F}
library("DescTools")
(mean(d$hotele2017) - Mode(d$hotele2017) )/ sd17  
```

Still the distribution is positively 
skewed, but the value of the coefficient is much smaller.


## Better tools: for producing better charts 

In spite of the fact that statistical charts are now ubiquitous in the media
this topic is usually covered marginally at most courses on statistics,
probably because it is pretty hard to produce quality graphics 
with office software (complexity vs difficulty).
This pitiful state 
of affairs can be changed with the introduction of modern tools. (How to make
quality graphics is the main subject of my lecture BTW.)

Statistical charts can be plotted for the following three purposes:

* **Decoration** (to attract somebodys' attention, document without pictures looks dull,
color pictures are better than back-white ones, fanciful drawings are better than simple ones,
*form is the king and content does not matter*)

* **Explanation** (to better explain some phenomenon to somebody. It is claimed that a picture is worth
thousand of words in this context)

* **Exploration** (looking for data patterns at the exploratory stage of data analysis)

Note: It is often recommended by some researchers 
to use charts at data cleaning stage of statistical analysis. I do not agree with it. Data cleaning can
be automated and should not relay nor on manual work nor on visual inspection. 
Using programs to check data 
is more efficient and reliable procedure. It is also 100% replicable contrary to visual inspection.

A visual-art designer not statistician is a right person for the 1st purpose. I am not an art-designer
so I will not tell you how to prepare eye-catching pictures. I am a statistician and I will
concentrate on *effective* graphical methods for statistical explanation/exploration. 
And by *effective* I mean that one (graphical) method  is more effective than another if its quantitative
information can be decoded more quickly/easily [Robbins 2005]

## Types of charts

Some graphs are better than others:

* **Recommended**: (ordered) dot plots, bar charts, lineplots, histograms and kernel
density estimates, stripcharts,
multipanel displays (instead of stacked bars multiple line/dot plots)
scatterplots (two variables)

* **Not recommended**: Pie charts, bubble charts, stacked bar charts,

Note: bar/line/pie charts were introduced by William Playfair in XVIII century.
Dot plots were introduced by John Cleveland (1980s).
Box-plots were introduced by John Tukey (1970s)

![](./WP.png)

More Playfair's charts can be found via google or at
[](http://www.math.usu.edu/symanzik/papers/2009_cost/editorial.html)


## Pie charts (not recommended), dot plots (recommended) and histograms (recommended)

[Nights spent by non residents](https://ec.europa.eu/eurostat/statistics-explained/index.php/Tourism_statistics#Nights_spent_by_non-residents_in_the_EU-28:_Spain_on_top)

```{r message=F, echo=F, warning=F}
library(ggplot2)
library("dplyr")
library(RColorBrewer)
library(ggpubr)

# https://www.datanovia.com/en/blog/how-to-create-a-pie-chart-in-r-using-ggplot2/
df <- data.frame( class=c("Spain", "UK", "Italy", "France", "Greece",
    "Austria", "Germany", "Croatia", "Portugal", "Netherlands", "Rest28"),
    n=c(305907462, 213378155, 210658786, 133499991, 89284386, 86044321, 83111219,
        80176804, 48884842, 44169041, 202635615));

sum.eu <-sum(df$n)
df["prop"] <- df$n / sum.eu * 100;
df["txt.prop"] <- sprintf ("%.1f", df$prop);

df <- df %>%
  arrange(desc(class)) %>%
  mutate(ypos = cumsum(prop) - 0.5*prop)

mycols <- brewer.pal(13, "Set3")

pc1 <- ggplot(df, aes(x = "", y = prop, fill = class)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  coord_polar("y", start = 0)+
  scale_fill_manual(values = mycols) +
  theme_void()

pc2 <- ggplot(df, aes(x = "", y = prop, fill = class)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = ypos, label = txt.prop), color = "black")+
  scale_fill_manual(values = mycols) +
  theme_void()

## dot-plot
pc3 <- ggplot(df, aes(x = reorder(class, n) )) +
  geom_point(aes(y = prop, colour = 'prop' ), size=3) +
  xlab(label="#") + ylab(label="% ") + coord_flip()+
  ggtitle("Share of...") +
  theme(plot.title = element_text(hjust = 0.5))

## bar chart
pc4 <- ggplot(df, aes(x = reorder(class, n), y=prop, fill=prop )) +
  geom_bar(stat="identity", color = 'blue' ) +
  xlab(label="#") + ylab(label="% ") +
  coord_flip()+ ggtitle("Share of...") +
  theme(plot.title = element_text(hjust = 0.5))

ggarrange(pc1, pc2, pc3, pc4, ncol = 2, nrow = 2)
```


## Interlude: what, when and where

Before we continue with statistical graphics there will be a short 2-slide interlude
on geocode standards used in statistics. 

No doubt in every reliable survey the population has to be precisely defined ie 
3 dimensions of every surveyed unit should be fixed:
definition (**what**), time (**when** measured), space (*where*)...

I always repet to my students: if you look at some data (in the media for example), start
from establishing if you know **what**, **when** and **where**. If no information (or 
*reliable link*--called source--to information) 
is provided
on any of the fixed dimensions of data, treat this data as rubbish and do not waste time to use/analyse it.

Further dissemination of such defective data should be subjected to publicly prosecuted (joke)

I tried to show you already that **what** is complicated and often highly unreliable/arbitrary
(the nature of the phenomenon or/and measurement difficulties).

**What** dimension much more simpler due to universal standard, ie. time. 
You gather data or for a certain moment (how many hotels
are in use in 31st December 2018) or for certain period of time (how many beds were sold in these hotels in 3rd quarter of 2018). 

**Where** dimensions in turn is usually based on administrative or statistical (geographical) units
(country, state/province, county, community). But contrary to time dimension
there is no universal or globally-accepted standard for **geostatistical** units.
Usually such a standard is based on administrative system which is country-dependent.

The administrative division of Poland since 1999 has 
been based on three levels of subdivision (cf [Administrative divisions of Poland](https://en.wikipedia.org/wiki/Administrative_divisions_of_Poland). 
In 2001 as Poland became a member of European Union, EU regulations are part of national law system.

EU regulates everything, statistics included.

Conclusion: The pigs had to expend enormous labours every day upon mysterious things called "files," "reports,"
"minutes," and "memoranda." These were large sheets of paper which had to be closely covered with writing,
and as soon as they were so covered, they were burnt in the furnace (George Orwell, Animal Farm)

## Interlude: NUTS and TERYT

The Nomenclature of Territorial Units for Statistics (NUTS)
is a geocode standard for referencing the subdivisions of countries for statistical purposes.
The standard is developed and regulated by the European Union, and thus only covers the member states of the EU in detail (cf [NUTS](https://en.wikipedia.org/wiki/Nomenclature_of_Territorial_Units_for_Statistics))

NUTS *standard* was revised several times (on the average every 4 years :-)), so there is even a page 
at ec.europa.eu domain dedicated to NUTS (short) history (cf 
[NUTS history](https://ec.europa.eu/eurostat/documents/345175/501899/Nuts-history))

![](./NUTS-history.jpg)

NUTS1 (level) -- macroregion, NUTS2 -- state, NUTS3 -- county
We would like to plot a chart showing number of hotels.

Poland is divided into 16 states (NUTS2) and 380 counties NUTS3 which are equal
to administrative units. So on the average there ar 23.75 counties per state.
NUTS1 level is only for statistical purposes (but regions are in fact
distinct due to history, economics, natural-conditions, cultural factors etc... )

There is a relevant and interesting page by GUS (Main Statistical Office or Główny Urząd Statystyczny),
but unfortunately in Polish (use google translate :-) in case you are interested or mail me)
(cf [Klasyfikacja NUTS w Polsce](https://stat.gov.pl/statystyka-regionalna/jednostki-terytorialne/klasyfikacja-nuts/klasyfikacja-nuts-w-polsce/)

![](./polska_nuts2-2016.png)

The above map shows 7 macroregions (NUT1) and 16 provinces (NUTS2).
BTW province in Polish is "prowincja" (due to both are from Latin) but actually 
Polish administrative provice is called "województwo", from "wodzić" -- ie commanding (the armed 
troops in this context).  This is an old term/custom from the 14th century, 
where Poland was divided into provinces (every province
ruled by a "wojewoda" ie chief of that province). More can be found
at Wikipedia (cf [Administrative divisions of Poland](https://en.wikipedia.org/wiki/Administrative_divisions_of_Poland)

NUTS3 consists of 380 counties (called "powiat"). In ancient Poland powiat was called
"starostwo" and the head of a "starostwo was called "starosta". "Stary" means Old, so
"starosta" is an old (and thus wise) person. 
BTW the head of powiat is "starosta" as 600 years ago:-)

![](./polska_nuts3-2016_r.png)

There is no NUT4 level but there is 3rd level of Polish administration used 
by GUS (Main Statistical Office). This 3rd level is called "gmina" (community). 

There are (approximately) 2750 communities in Poland. 
As Poland population is 38,5 mln and the area equals 312,7
sq kilometers (120 persons per 1 sqkm) on the average each powiat has 820 sqkm and each community
has 113.5 sqkm or approximately 100 thousand persons per "powiat and 14 thousand per "gmina".

TERYT is a Polish NUTS (developed some 50 years ago). It is complex system which includes
identification of administrative units. Every unit has (up to) a 7-digit id number: wwppggt
where ww = "województwo" id, pp = "powiat" id, gg = "gmina" id and "t" decodes type-of-community
(rural, municipal or mixed). Higher units has trailing zeros for irrelevant part of id, so
14 or 1400000 means the same; as well as 1205 and 1205000. Six numbers is enough to identify
a community (approx 2750 units).

So you are now experts on administrative division of Poland, and we can
go back to statistical charts...

## Strip charts

A strip chart (strip plot) shows *the distribution* of data points along
a numerical axis.These plots are suitable compared 
to box plots when sample sizes are small (because preserve more information about the data).

Example: Number of hotels in powiat by region (NUTS1, 2017):

```{r, cache = F, echo=F, dependson = "pl_hotel_data"}
d <- read.csv("powiaty_wskazniki_woj_regiony.csv", sep = ';',  header=T, na.string="NA");
d$wteryt <- as.factor(d$wteryt)

p4<-ggplot(d, aes(x=nuts1, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.0), cex=.2)
p3<-ggplot(d, aes(x=nuts1, y=hotele2017)) +
  geom_point(alpha = 1/20)
  #geom_point(size = .3)
  #geom_jitter(position=position_jitter(0.05), cex=1.2)
ggarrange(p4,p3, ncol = 2, nrow = 1)
```

The biggest potential problem with a dot/scatterplot is overplotting: 
whenever one has more than a few points, points may be plotted on top of one another. 
This can severely distort the visual appearance of the plot (left panel)

There is no one solution to this problem, but there are some techniques that can help: 
use smaller dots, use semi-transparent dots (right panel), use jitter.

Jitter---a small random noise added to data, is shown below
(higher jitter on the right panel)

```{r, cache = F, echo=F, dependson = "pl_hotel_data"}
p1<-ggplot(d, aes(x=nuts1, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.2), cex=.2)
p2<-ggplot(d, aes(x=nuts1, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.1), cex=.2)
ggarrange(p2,p1, ncol = 2, nrow = 1)
```

## Histograms and kernel density functions

Histograms show the distribution of a set of data. To draw a histogram
the numbers (observations) are grouped
into bins (intervals or classes). There is
a trade-off between showing details or showing an overall picture.
When bin width changes the scale at Y-axis changes as well (more bins less
observations in each bin). Example number of hotels in Poland (2017):

```{r, cache = F, dependson = "pl_hotel_data"}
ggplot(d, aes(x = hotele2017)) +
  geom_histogram(bins = nclass.Sturges(d$hotele2017))
```

Histograms with binwidth equal to 20, 10, 5 and 1 respectively:

```{r, cache = F, echo=F, dependson = "pl_hotel_data"}
#library(ggpubr)
p1 <- ggplot(d, aes(x = hotele2017)) +
  geom_histogram(binwidth = 20)
p2 <- ggplot(d, aes(x = hotele2017)) +
  geom_histogram(binwidth = 10)
p3 <- ggplot(d, aes(x = hotele2017)) +
  geom_histogram(binwidth = 5)
p4 <- ggplot(d, aes(x = hotele2017)) +
  geom_histogram(binwidth = 1)

ggarrange(p1,p2,p3,p4)
```

Kernel density functions

```{r, cache = F, echo=T, dependson = "pl_hotel_data"}
ggplot(data=d) + geom_density(aes(x=hotele2017))
```

```{r, cache = F, echo=T, dependson = "pl_hotel_data"}
p1 <- ggplot(data=d) + geom_density(aes(x=hotele2017), adjust=0.25)
p2 <- ggplot(data=d) + geom_density(aes(x=hotele2017), adjust=1.0)
p3 <- ggplot(data=d) + geom_density(aes(x=hotele2017), adjust=2.0)
p4 <- ggplot(data=d) + geom_density(aes(x=hotele2017), adjust=8.0)
ggarrange(p1,p2,p3,p4)
```

## Comparing distributions: box-plots

Box-plots are much better than histograms for comparing distributions
of more than one data sets.

Construction of a (typical) box-plot: The middle bar is a median. Top/bottom
bars of the rectangle shows the IQR (interquartile range is 1st and 3rd\
quartile), the fanciful bars above/below rectangle called whiskers
(google: whiskers mustache :-) are 1,5 times the IQR (or minimu/maximum if
those values are  less than plus/minus 1,5 IQR.
The symbols above/below whiskers (usually open circles)
are outliers (non typical/extreme values)

Note the trick: outliers are defined not as (for example) top/botom
1% fraction of values (every distribution would has outliers in such a case)
but as values less/more than Me - 1,5IQR (distributions with medium
variablity would not have outliers)

Example: age of Nobel-prize 
winners (cf [The Nobel Prize API Developer Hub](https://nobelprize.readme.io/))

```{r, cache = F, echo=T }
nlf <- read.csv("nobel_laureates3.csv", sep = ';', dec = ",",  header=T, na.string="NA");

ggplot(nlf, aes(x=category, y=age, fill=category)) + geom_boxplot() + ylab("years") + xlab("");
```

Multiple histograms are too detailed (binwidth=5). It is impossible for example to establish
which category has the youngest (on the average) laureate, or which category 
has an oldest one (economics 
and literature are candidates, but due to multimodality of literature laureates
distribution it is difficult to assess this for sure...)



## Comparing distributions box-plots vs multiple histograms

Number of hotels in powiat by województwo (2017):

```{r, cache = F, echo=F, dependson = "pl_hotel_data"}
p3<-ggplot(d, aes(x=wteryt, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.05), cex=.3)
p4<-ggplot(d, aes(x=wteryt, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.0), cex=.3)
ggarrange(p4,p3, ncol = 2, nrow = 1)
```

More jitter:

```{r, cache = F, echo=F, dependson = "pl_hotel_data"}
p2<-ggplot(d, aes(x=wteryt, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.1), cex=.3)

p1<-ggplot(d, aes(x=wteryt, y=hotele2017)) +
   geom_jitter(position=position_jitter(0.2), cex=.3)
ggarrange(p2,p1, ncol = 2, nrow = 1)
```

Boxplots are better:

```{r, cache = F, echo=F, dependson = "pl_hotel_data"}
p5 <- ggplot(d, aes(x=wteryt, y=hotele2017, fill=wteryt)) + geom_boxplot() + ylab("#") + xlab("");

p6 <- ggplot(d, aes(x=wteryt, y=hotele2017, fill=wteryt)) + geom_boxplot(outlier.shape=NA) + ylab("#") + xlab("") +
      coord_cartesian(ylim = c(0, 25)) +
      theme(legend.position="none")
ggarrange(p5,p6,ncol = 2, nrow = 1)
```

## Scatter-plots

A scatter-plot (aka scatter diagrams, xyplot) is a basic
form used for two (quantitative) variables.

To see the relationship between variables, a line is can be fitted.
Least square (LS) line which assumes linear relationship
between variables, is fitted by minimizing the sum of squares of the residuals
(residual is the difference between
a data-point and a relevant line-point ie a point computed from the formula
y = a +bx where x is the value of the x-axis variable.)

(Almost) each part of Poland is attractive for tourists, but those counties which are at the seaside (north) 
or in the mountains (*south*) are special. There are 11 counties at the seaside (morze = sea)
and 18 in the mountains (*góry*):


```{r, cache = F, echo=F, warning=FALSE}
d <- read.csv("powiaty_hotele_caloroczne_turysci_2017M.csv", sep = ';',  header=T, na.string="NA");

d <-  subset(d, (y2017 > 0 ))
dd <- d;

m <- subset(dd, (teryt %in% c("3263", "3207", "3205", "3208", "3209", "3213", "2212", "2208", "2215", "2211", "2210")));
# szacujemy prosty model trendu
lm <- lm(data=m, tz2017 ~  y2017 ); summary(lm)
otherc <- coef(lm);
# W tytule średnia/mediana i równanie trendu
title <- sprintf ("Hotels vs foreign tourists morze 2017 (y = %.2f x + %.1f)", otherc[2], otherc[1] );
q0h <- ggplot(m, aes(x=y2017, y=tz2017)) + geom_point() +
 ggtitle(title) +
 geom_smooth(aes(x = y2017, y=tz2017, colour="tz2017"), method="lm", size=.5)

m <- subset(dd, (teryt %in% c("1801", "1821", "1817", "1807",
  "1805", "1205", "1210", "1211", "1217", "1215", "2417", "2403", "0208", "021", "0206", "0212")));
# szacujemy prosty model trendu
lm <- lm(data=m, tz2017  ~ y2017 ); summary(lm)
otherc <- coef(lm);
# W tytule średnia/mediana i równanie trendu
title <- sprintf ("Hotels vs foreign tourists góry 2017 (y = %.2f x + %.1f)", otherc[2], otherc[1] );
q1h <- ggplot(m, aes(x=y2017, y=tz2017)) + geom_point() +
 ggtitle(title) +
 geom_smooth(aes(x = y2017, y=tz2017, colour="tz2017"), method="lm", size=.5)

# Loess

m <- subset(dd, (teryt %in% c("3263", "3207", "3205", "3208", "3209", "3213", "2212", "2208", "2215", "2211", "2210")));
title <- sprintf ("Weekly for %s # (y = %.2f x + %.1f)", "", otherc[2], otherc[1] );
q2h<- ggplot(m, aes(x=y2017, y=tz2017)) + geom_point() +
 ggtitle("Hotels vs foreign tourists morze 2017") +
 geom_smooth(aes(x = y2017, y=tz2017, colour="darkblue"), method="loess", size=.5)

m <- subset(dd, (teryt %in% c("1801", "1821", "1817", "1807",
  "1805", "1205", "1210", "1211", "1217", "1215", "2417", "2403", "0208", "021", "0206", "0212")));
q3h<- ggplot(m, aes(x=y2017, y=tz2017)) + geom_point() +
 ggtitle("Hotels vs foreign tourists 2017 (góry)") +
 geom_smooth(aes(x = y2017, y=tz2017, colour="darkblue"), method="loess", size=.5)


ggarrange(q0h,q1h, ncol = 2, nrow = 1)
```

So each new hotel in the mountains on the average would attract 961.6 foreign tourists,
while a new hotel at the seaside would attract
5838 foreign tourists (and both numbers are statistically significant at $\alpha=0.05$:-) )


Alternatively loess curve can be used which do not assumes linearity but is parameters
are not interpretable.

```{r, cache = F, echo=F, warning=FALSE}
ggarrange(q2h,q3h,ncol = 2, nrow = 1)
```

## Scales

Logarithmic scale makes it possible to plot values with too
wide range for a linear scale. Base 10 logarithms `squeeze' the numbers
more than base 2 logarithms (log10(100)=2 wile log2(100)=6.64.
Moreover if the original scale contains multiplications of 10 use
log10 to get `nice' log-scale while it
contains multiplications of 2 use log2.

Logarithms transforms additive scale to `multiplicative' one. Example (Nobel prize again):

```{r, cache = F, echo=T }
dA <- read.csv("nobel_laureates3.csv", sep = ';', dec = ",",  header=T, na.string="NA");
nrow(dA)
dS <-  subset(dA, (! bornCountryCode == "" )) # by country of birth
nrow(dS) # how many
```

aggregate by bornCountryCode

```{r, cache = F, echo=F }
u <- table(dS$bornCountryCode)
uf <- as.data.frame(u)
```
Finally plot the resulting data using various Y-axis scales (arithmetic, log2 and log10)

```{r, cache = F, echo=F }
names(uf)[1] = 'country'

lFreq <- log2(uf$Freq)
uf["lfreq"] <- lFreq
pc3 <- ggplot(uf, aes(x = reorder(country, Freq) )) +
  geom_point(aes(y = Freq, colour = 'Freq' ), size=1) +
  xlab(label="cc") + ylab(label="n ") + coord_flip()+
  ggtitle("Number of Nobel laureates by Country") +
  theme(axis.text = element_text(size = 7)) +
  theme(plot.title = element_text(hjust = 0.5))
pc4 <- ggplot(uf, aes(x = reorder(country, lfreq) )) +
  geom_point(aes(y = lfreq, colour = 'lfreq' ), size=1) +
  xlab(label="cc") + ylab(label="log(n) ") + coord_flip()+
  ggtitle("Number of Nobel laureates by Country") +
  theme(axis.text = element_text(size = 7)) +
  theme(plot.title = element_text(hjust = 0.5))

llFreq <- log10(uf$Freq)
uf["llfreq"] <- llFreq

pc5 <- ggplot(uf, aes(x = reorder(country, llfreq) )) +
  geom_point(aes(y = llfreq, colour = 'lfreq' ), size=1) +
  xlab(label="cc") + ylab(label="log(n) ") + coord_flip()+
  ggtitle("Number of Nobel laureates by Country") +
  theme(axis.text = element_text(size = 7)) +
  theme(plot.title = element_text(hjust = 0.5))

ggarrange(pc3, pc4, pc5, ncol = 3, nrow = 1);
```

The exact figures are as follows:

```{r cache = F, echo=F }
u
```
## Graphic perception tasks

From the best to the worst:

* Position along common scale

* Position along common but nonaligned scales

* Length

* Angle (slope)

* Area

* Volume

* Color (hue), Color (saturation), Color (density of black)

Angle judgement is not precise. Acute angles are underestimated
while obtuse angles (greater than 90) are  overestimated.

Area judgement is biased as well. It is impossible to distinguish
small differences in area, while quite easy when the same date
is plotted along common scale

The most accurate of graphic task is 
positioning along common scale

## General design rules

* Clear content: Reader/receiver/consumer clearly *understands* what is graphed:
scales/labels/explanations are provided (remember what/when/where?) 

* Clear form: reader/receiver/consumer clearly sees what is graphed
(no cluttered lines, overlapping elements, etc...)

* Emphasize the data not grids, labels or pointless arrows. The simpler
the better, leave complicated designs for professionals. For example
use gray not black ink (default in Excel) for grid lines 

* Tick marks and axis labels should be placed outward. X-axis values increase
always from left to the right, Y-axis values from the bottom to the top
**never** in reverse direction. Do not overdo the number of tick marks.

* Preparing color chart think how it will look like when reproduced in B-W
(xerox) or half-size or less (smartphone). 
It is important particularly in electronic print.

* Never use **more** graphic feature than your data set has dimensions.
For univariate analysis use length or color not both for example.
(Well rare exceptions to this rule are allowed)

* Pseudo 3D charts for 2D data should be forbidden as well and without
any exception. Virtually no-one can read them. 

* Use a common baseline wherever possible. Use optimum aspect ratio (banking to 45, see below). Use logaritmic scales when data range is huge, do not break
scales and generally always include 0 in numerical 
axes (not 100% obligatory however.) 
Do not (generally) use double axes.

* Prefer direct labels over using separate legend. Separate legend
forces the reader to look back and forth when studying the graph.
Of course if there is no room for (long) labels use legend.

* Multiline graphs generally are bad idea (different scales, clutter,
difficulty with assessing the difference between lines)

* Do not use crappy software which produces charts in proportion to data

* Tufte (a renown expert on Information Visualization) coined two popular
rules: (high) data to ink ratio and a lie-factor.

Ink in this definition  refers to non-erasable ink used for 
the presentation of data. If data-ink would be removed from the image, the graphic would lose the content. Non-Data-Ink is accordingly the ink that does not transport the information but it is used for scales, labels and edges.

Good graphics should include only data-Ink. Non-Data-Ink is to be deleted everywhere where possible. The reason for this is to avoid drawing the attention of viewers of the data presentation to irrelevant elements. There is an
short an excellent video clip at YouTube which illustrates this rule.

[Data to Ink Ratio](https://www.youtube.com/watch?v=JIMUzJzqaA8)

Lie factor (LF) is a ratio as well but defined as size of the effect shown in graphics
to the size of effect in data. Preferaby LF should equal 100%. According
to Tufte, LF greater than 1.05 or less than 0.95 signals significant
distortion. This rule can be best explained with an example.

## Lie factor example

This *giant guy* (GG) in the middle is our _ex-president_. The guy next to him on the left
is our *current president* Duda. Next to Duda is ex-rock star Kukiz, dark-horse of the elections.
This is the cover (slightly modified) 
of influential polish weekly magazine form May 2015, shortly before elections.

The figures are claimed to be in-sync with the recent survey results (sort of a barchart). 
Could you figure-out from that chart about the proportion of scores of each candidate?
How much the giant-guy outperforms the runner-up candidate? Which candidate is supported
by this influential magazine (easy:-)?

![](./Komorowski_Duda_Kukiz_sondaz_Polityka_hidden_numbers.jpg)

The lie-factor details:

![](./Komorowski_Duda_Kukiz_sondaz_Polityka_flactor_x.jpg)

The line from shoes to top of the head equals (at certain size of course) 204mm for GG, 134mm for Duda
and 42.5mm for ex-rock star. 
So $204/134=1.5$ and $204/42.5 \approx 4.8$. As $44/29 \approx 1.5$
and $44/9 \approx 4.8$ as well formally the lieFactor is perfect. 
But should one compares lengths or areas?

If one compares **areas** not heights, one get significantly different (and correct)
results, namely: $(204 * 58) /(134 * 21)= 4.20$ and $(204 *58)/(42.5 *15) \approx 18.56$. Lie factor
is $4.2/1.5 =280$% and $18.56/4.8=387$% respectively. Huge distortion

Moreover two more tricks were applied to boost GG. Can you see them?

BTW: the text in the pink frame claims: "figure ratios are consistent with april-may survey outcome.""
(But what exactly *figure ratios* means?)

## Banking to 45

The ratio between the width and the height of a rectangle is called
its aspect ratio.

The aspect ratio describes the area that is occupied by the data in
the chart. A change in aspect ratio changes the perception
of the graph. The question is which aspect ratio is the best.

We can recognize change  most easily if absolute slopes
equals to 45 degree angle on the graph. It is much
harder to see change if the curves are nearly horizontal/vertical.
The idea (Cleveland, 1988) behind banking is therefore to adjust
the aspect ratio of the entire plot in such a way that most slopes are
at an approximate 45 degree angle.

Setting the aspect ratio so that the average of the values of the
orientations is 45 degrees is called "banking the average
orientation to 45 degrees".

Setting the aspect ratio so that the weighted mean of line segments
(weighted by segments' length is approx 45 degrees is called average
weighted orientation method (to 45 degrees).

**Exercise**: assess which slope is the steepest one and which is the smallest one?

![](./SameSlopeRate.jpg){width=60%}

BTW: every chart presents the same data 
on CO2 emission (average for May each year) 
as provided by US Government's Earth 
System Research Laboratory, Global Monitoring Division. 
(cf [CO2 PPM - Trends in Atmospheric Carbon Dioxide](https://datahub.io/core/co2-ppm))

## Elementary spatial analysis (Heat maps/choropleth  maps)

I do not intend to give you a full lecture on spatial methods/analysis now.
First of all I am not an expert in this area.  Second most of the
methods develops by cartographers are not used in the domain of
social-sciences. But a few are **popular**, are **simple** and are pretty
**impressive** (to family and friends at least (F&F), ie. for
non-professionals) Why not to use them?  (To make impression on your
F&F and/or your boss?). These methods are:

* choropleth map 

* feature map

* heat map

A choropleth map is a thematic map where geographic regions are
colored, shaded, or patterned in relation to a value.

Feature map is a map augmented with position of an **object** of interest
somehow marked.

A heat map represents the intensity of **object**s occurrence within
a dataset. A heatmap uses color to represent intensity, though unlike
a choropleth map, a heatmap does not use geographical or geo-political
boundaries to group data. 
This technique requires point geometries, as
you are looking to map the frequency of an occurrence at a specific
point.

One can think of choropleth map as *a kind of* spatial histogram,
while feature map (heat map) is *a kind of* spatial dot-plot
(fanciful spatial dot-plot).

## Spatial analysis tools 

Google Geoservices are now non-free if used not "directly" but with API. 
One have to register an 
credit/debit card and sign some obscure license to use them. I used
to use Google for years but stop using them last year.

Google shut down some cool geo-services including Google Fusion Tables
(launched 10 years ago). I was a big fan of GFT and I am greatly
disappointed about the decision to shutting them down now.

QGIs is a full-featured, matured (2002) and powerful open source 
geographic information system (GIS) software.

It allows to analyze and edit spatial
information, in addition to composing and exporting
graphical maps. QGIS supports both raster and vector
layers; vector data is stored as either point, line, or polygon
features. Multiple formats of raster images are supported,
and the software can georeference images.

QGIS supports shapefiles, PostGIS (ie the most important ones), and other
formats. Web services, including Web Map Service and Web
Feature Service, are also supported to allow use of data
from external sources (Open Street Map for example).

QGIS integrates with other open-source GIS packages,
including PostGIS, GRASS GIS, and MapServer. Plugins
written in Python or C++ extend QGIS's capabilities.

To start with QGIS simply go to
[www.qGIS.org](http://www.qGIS.org/en/site/forusers/download.html),
download it and start installtion. No need to learn the
whole system. Being (somehow) acquainted with Project
and Layer/Add Layer menu item is enough:

* Project: it manages file, opening, saving, printing (maps).
Every program has such an top-menu-item usually called File

* Layer: it allows adding, removing, coloring, lay outing for
map layers taken from different data files. In particular 
it contains Layer -> Add Layer menu item which is the only one I use.

## Example: Poland (population, incomes, distribution of)

A CSV file `PL_powiaty_2017.csv` which I compile for this
lecture contains cross-sectional data
for every Polish powiat (generally for 2017.) Among other things
one can find there:

* teryt, wteryt, nuts1  (powiat identifiers explained above)

* basic information on area and population (areaH, areakm, pop, popkm)

* number of hotels (hotele2012, hotele2017)

* number of high schools/graduates (wSzkoly, absolwenci)

* number of companies, their aggregated income and profit from Rzeczpospolita 2000 ranking (firmyRz, przychodRz, wynikNettoRz)

* "powiat revenue per inhabitant (as transferred by central government, przychodMF). This information is publicly available (distributed by
Polish Financial Minister) contrary to GDP per powiat for example which is nota available. Powiat's revenues 
are indicators of powiat economic strength, poor powiats has low transfers
while a rich one has high...

```{r, cache = F, echo=T, warning=FALSE, error=FALSE }
d <- read.csv("PL_powiaty_2017.csv", sep = ';',  header=T, na.string="NA");

revF <- fivenum(d$przychodMF)
revM <- mean(d$przychodMF, na.rm=T)
revD <- sd(d$przychodMF, na.rm=T)

c(revF, revM, revD)

ggplot(d, aes(x = przychodMF)) + geom_histogram(bins = nclass.Sturges(d$przychodMF))
ggplot(d, aes(x = przychodMF)) + 
  geom_histogram(binwidth = 40) # about 10 USD (as of march 2019)
```

So on the average the revenues was `r sprintf("%.2f", revM)` and
the relative dispersion `r sprintf("%.2f", revD/revM *100)`% złotych 
(fortunately
Poland do not "join Euro area", and we still use local currency called złoty; złoty means literally "made of gold" BTW). Half of powiats' revenues
was between `r sprintf("%.2f", revF[2])` 
złoty and `sprintf("%.2f", r revF[4])` złoty 
or PLN (Q1/Q3) 
with `r sprintf("%.2f", revF[1])` PLN minimum 
and `r sprintf("%.2f", revF[5])` PLN maximum incomes respectively.

To understand the spatial distribution of wealth one can plot
choropleth map (using QGIS not R):

![](./DotacjaP0.jpg)

Number of high schools

![](SzkolyP0.jpg)

Number of hotels

![](./HoteleP0.jpg)

population density (number of people per kilometer square)

![](./PopulacjaP0.jpg)


More examples can be found at my Github account (URL 
will be provided on the last slide)

## Example: UNESCO World Heritage List

A World Heritage (WH) Site is a place that is listed by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as having special cultural or physical significance. There is a inventory of WH sites
at [whc.unesco.org](https://whc.unesco.org/en/syndication).
This list are available in various formats including Excel format
and when rendered with QGIS looks like:

![](./UHL.jpg)

![](./UZ-UHL.jpg)

Heatmaps shows density more clearly (or not--opinions are contradictory):

![](./UHL-12.jpg)

## Example: Nobel prize winners by place of birth

Remember Nobel winners by country? With heat-maps one can 
plot them on the map:

![](./NTL-1234.jpg)

## Example: Concentration of Polish big industry

Every year [Rzeczpospolita](https://en.wikipedia.org/wiki/Rzeczpospolita_(newspaper)),
a nationwide daily economic and legal newspaper 
compiles a list of 2000 biggest companies (idea similar to Fortune 500 list)
The distribution is highly skewed and concentrated, but what about spatial
distribution of Polish big companies? Feature/Heat to the rescue...

![](./Rz2000-12g.jpg)

BTW: the small picture in the middle depicts Poland during
Weichselian and Würm cold period (15,000--11,700 years ago, 
cf [Weichselian glaciation](https://en.wikipedia.org/wiki/Weichselian_glaciation))

## Interlude: Crusaders, Knights and Malbork castle

First short explanation about the subject of the analysis ie
famous Castle of the Teutonic Order in Malbork which is enlisted at UNESCO heritage list
(cf [UNESCO heritage list](https://whc.unesco.org/en/syndication) ):

Several religious military orders were formed in the Holy Land during the Crusades
Templars, Hospitallers, Teutonic Knights

The Teutonic Knights or the Teutonic Order of the Hospital of St. Mary
in Jerusalem, were known in Poland as *Krzyżacy* on account of the black
cross they wore on their white coats. 

Established in 1190 to protect
German pilgrims in the Holy Land, the order was later transformed in
order to fight heretics.

In 1226 the Teutonic Knights came to Poland, invited by
Duke Konrad I of Mazovia to fight with the annoying pagan Prussian tribes
invading Poland from time-to-tme from the north.
Teutonic Knights conquered Prussia, exterminated the locals and founded a powerful 
state with Malbork (Marienburg or Mary's castle in German) as its capital.

BTW: Kwidzyn in German is called Marienwerder (Mary's meadow) 
and there were a lot more places named Marien-something  (as Marien is St Mary in German)

BTW2: There is about 40km from Kwidzyn to Malbork :-)

## Interlude: example of a very bad graphs

There is a research, peer-reviewed paper on tourist traffic in the castle's museum of Malbork

[The determinants of the tourist traffic
 in the castle's museum of Malbork](http://www.ojs.ukw.edu.pl/index.php/johs/article/view/4119/pdf)

Unfortunately all charts in this paper contains elementary errors. Could you identify them?

![](./MBC_1.png)

![](./MBC_2.png)


```{r, cache = F, echo=F, warning=FALSE, error=FALSE}
#library(ggplot2)
#library(RColorBrewer)
#library(dplyr)
#library(ggarrange)

df <- data.frame( visit.freq=c("every few years", "once a year", "every few months", "once a month", "every few weeks"),
n=c(36,30, 22, 6, 6))

colors <- brewer.pal(10, "Set1");
df["label"] <- sprintf ("%.1f", df$n);

df <- df %>%
  arrange(desc(visit.freq)) %>%
  mutate(ypos = cumsum(n) - 0.5*n)

px1 <- ggplot(df, aes(x = "", y = n, fill = visit.freq)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = ypos, label = label), color = "black", size=3)+
  scale_fill_manual(values = colors, name="Frequency") +
  theme_void()

px2 <- ggplot(df, aes(x = reorder(visit.freq, n), y=n)) +
  geom_bar(stat="identity", fill = 'steelblue' ) +
  xlab(label="#") +
  ylab(label="% ") +
  coord_flip()+
  geom_text(aes(label=n), hjust=1.6, color="white", size=3.0)+
  ggtitle("Frequency...") +
  theme(plot.title = element_text(hjust = 0.5))

dg <- data.frame( visit.freq=c("1--2", "3--5", "6--7", "8 and more"),
n=c(78.9, 18.0, 2.1, 1.0))
dg["lab"] <- sprintf ("%.1f", dg$n);

dg <- dg %>%
  arrange(desc(visit.freq)) %>%
  mutate(ypos = cumsum(n) - 0.5*n)

px88 <- ggplot(dg, aes(x = "", y = n, fill = visit.freq)) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  coord_polar("y", start = 0)+
  geom_text(aes(y = ypos, label = lab), color = "black", size=3)+
  scale_fill_manual(values = colors, name="Frequency") +
  theme_void()

px9 <- ggplot(dg, aes(x = reorder(visit.freq, n), y=n)) +
  geom_bar(stat="identity", fill = 'orange' ) +
  xlab(label="#") +
  ylab(label="% ") +
  coord_flip()+
  geom_text(aes(label=lab), hjust=-.3, color="darkgreen", size=3.0)+
  ggtitle("Length of stay (days)") +
  theme(plot.title = element_text(hjust = 0.5))

dh <- data.frame( visit.freq=c("guestrooms", "family", "camping", "hostel", "hotel", "guesthouse"),
      n=c(48.0, 23, 11, 9.0, 5.4, 3.6))

dh["lab"] <- sprintf ("%.1f", dh$n);

px3<-ggplot(dh, aes(x = reorder(visit.freq, n), y=n)) +
  geom_bar(stat="identity", fill = 'orchid2' ) +
  xlab(label="#") +
  ylab(label="% ") +
  coord_flip()+
  geom_text(aes(label=lab), hjust=-.1, color="darkgreen", size=3.0)+
  ggtitle("Type of accomodation") +
  theme(plot.title = element_text(hjust = 0.5))


px4<-ggplot(dh, aes(x = reorder(visit.freq, n) )) +
  geom_point(aes(y = n, color="blue",show.legend=F), size=3) +
  xlab(label="#") +
  ylab(label="% ") +
  coord_flip()+
  geom_text(aes(y=n, label=lab), hjust=.1, vjust=-.8, color="darkgreen", size=3.0)+
  ggtitle("Share of...") +
  scale_color_discrete(guide=F) +
  theme(plot.title = element_text(hjust = 0.5))

```

if one insists on using piecharts (improved version):

```{r, cache = F, echo=F, warning=FALSE, error=FALSE}
ggarrange(px1, px88, ncol = 2, nrow = 1);
```

or better, using bar/dot charts:

```{r, cache = F, echo=F, warning=FALSE, error=FALSE}
ggarrange(px2, px3, px4, px9, ncol = 2, nrow = 2);
```

## Even worse graphics (yes we can:-) )

Piecharts are notorious for obscurity:

![](./budzety_piechart.jpg)

![](./reklama_prasy_piechart.jpg)

What about this barchart (distribution of seats in Polish parliament (Sejm) after 2015 
elections---50% majority is 430 seats)?

![](./koszmarny_barchart.jpg)

Remember dark-horse ex-rock start Kukiz? IMO his bar does 
not looks like being equal to 50 votes (minus 1.)
PO-bar is peculiar as well...

Not mention about strange tilt to the left...


## New workflow (finally): reproducible research

Sorry but why use all this strange stuff at all?

So you probably still wander why I am punishing myself with using such a odd system. The
most important argument why I will present momentarily and it concerns the basic
approach (philosophy if one has to be pompous) 
of doing statistical analysis. 

This mode (or concept) is called Reproducible Research (RR in short).

Serious statistical analysis is not one-off job. There is a *value-chain* as well as 
a *life cycle* of statistical analysis. *Value chain* means that there are
distinct stages while *life cycle* that the same data/models are used for years and
most statistical analysis do not start from the scrach but are based on data from 
the past augmented with new data. 
The problem is that the new data and model modifications should be 
in-sync with the past. 
 
The make the problem worse, serious statistics should be also
in-sync with the work of others (to ease or to make 
possible any meaningful (international) comparisons for example)

## Reproducible research or how to make statistical computations more meaningfu
    
Abandoning the habit of secrecy in favor of process transparency and
peer review was the crucial step by which alchemy became chemistry.
Eric S. Raymond, E. S. The art of UNIX programming: Addison-Wesley.

Replicability vs Reproducibility

Hot topic: google: reproducible research = 158000

**Replicability**: independent experiment targetting the same question
will produce a result consistent with the original study.
  
**Reproducibility**: ability to repeat
the experiment with exactly the same outcome as
originally reported [description of method/code/data is needed to do so].

Computational science is facing a credibility crisis: it's impossible
to verify most of the computational results presented at conferences
and in papers today. (Donoho D. et al 2009)

## Australopithecus (Current practices)

* Enter data in Excel/OOCalc to clean and/or make explanatory analysis.

Use Excel for data cleaning & descriptive statistics
Excel handles missing data inconsistently and sometimes incorrectly
Many common functions are poor or missing in Excel

* Import data from Spreadsheet into SPSS/SAS/Stata for serious analysis

Use SPSS/SAS/Stata in point-and-click mode to run serious
statistical analyses.

* Prepare report/paper: copy and paste output to Word/OpenOffice, add
description.

* Send to publisher (repeat 1--4 if returned for revision).
  
Problems

Tedious/time-wasting/costly.

Even small data/method change requires
extensive recomputation effort/careful report/paper revision and update.

Error-prone: difficult to record/remember a 'click history'.

Famous example: Reinhart and Rogoff controversy
Countries very high GDP--debt ratio suffer from low growth. However the study
suffers serious but easy identifiable flaws which were discovered when
RR published the dataset they used in their analysis
(cf [Growth_in_a_Time_of_Debt](https://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt))
  
## Homo habilis (Enhanced current practices)

* Abandon spreadsheets.

* Abandon point-and-click mode. Use statistical scripting
languages and run program/scripts.

Benefits

Improved: reliability, transparency, automation, maintanability.
Lower costs (in the long run).

Solves 1--2 but not 3--4.

Problems: Steeper learning curve.
Perhaps higher costs in short run.
Duplication of effort (or mess if scripts/programs are poorly documented).

## Homo Erectus (Literate statistical programming)

Literate programming concept:
Code and description in one document. Create software as
works of literature, by embedding source code inside
descriptive text, rather than the reverse (as in most programming
languages), in an order that is convenient for human readers.

A program is like a WEB tangled and weaved (turned into a document),
with relations and connections in the program parts.  We express a
program as a *web of ideas*.  WEB is a combination of
-- a document formatting language and -- a program language.

General idea of Literate statistical programming mimics Knuth's WEB system.

Statistical computing code is embedded inside descriptive
text. Literate statistical program is weaved (turned) into
report/paper by executing code and inserting the results
obtained. data/method changes.

Solves 1--4.

## LSP: Benefits/Problems/Tools

* Reliability: Easier to find/fix bugs.
The results produced will not change when recomputed (in theory at least).

* Efficiency: Reuse allows to avoid duplication of effort (Payoff in the long run.)

* Transparency: increased citation rate, broader impact, improved institutional memory

* Institutional memory is a collective set of facts, concepts, experiences and know-how 
held by a group of people. 

* Flexibility: When you don't 'point-and-click' you gain many new analytic options.

Problems of LSP: Many incl. costs and learning curve

Tools:

* Document formatting language: LaTeX (not recommended) or Markdown (or
many others, ie. orgmode).  LaTeX is a word processor/a document
markup language.  Markdown: lightweight document markup language based
on email text formatting. Easy to write, read and publish as-is.

* Program language: R

## Interlude: Github for the uninitiated

The basic idea is that instead of manually registering 
changes one has made to data, documents etc, one can use software to
help him manage the whole process.
Such software is called **Version Control Systems** or VCS

VCS not only manages content, registering each modification of it, but 
control access to the content as well. Thus many individuals can
work on common project (compare this to common scenario of mailing
spreadsheets to each other--highly inefficient at least)

There are highly reliable and publicly available VCS services
and GitHub is the most popular of them.

GitHub is owned by Microsoft (do not use if you boycott MS :-))

I use GitHub as an educational tool: to distribute learning content
to my students and to store content they produce for me (ie projects)

The free GitHub account is public. It is OK for me. If it is not OK
for you, you can buy a license for commercial account or
do not use GitHub.

## Summary: New Tools (hipster part)

* R/Rstudio for computing and data visualization

* Github for enhancing team work

* markdown for reproducible research

* some other tools: QGIS for example

## Summary New practice, learning resources and data banks

**New practice**

* Introduce reproducible research approach

* Use real (big and dirty) data sets.

* Introduce some programming (Programing or using mouse?)

* Introduce some new tools (R/Rstudio, QGIS, Github)

**Learnig resources**

* [Rstudio](https://www.rstudio.com/resources/cheatsheets/)

* [Making Data Meaningful](https://www.unece.org/stats/documents/writing/)

* [bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/)

* Supplementary resources 
to my lecture (slides/data/R scripts etc) 
  are available at:
[https://github.com/hrpunio/Z-MISC/tree/master/Erasmus/2019/Namagan](https://github.com/hrpunio/Z-MISC/tree/master/Erasmus/2019/Namagan)


**Data banks**

* [Polish Main Statistical Office](https://stat.gov.pl/)

* [Bank Danych Lokalnych (Local Data Bank)]()https://bdl.stat.gov.pl/BDL/start)

* [Eurostat (European Union Statistical Office)](https://ec.europa.eu/eurostat/data/database)

* [My github repository](https://github.com/hrpunio))

* [Tourism](http://appsso.eurostat.ec.europa.eu/nui/submitViewTableAction.do)

* [Coffee production/consumption](https://www.kaggle.com/sbajew/icos-crop-data)

## Geo resources

* [GISCO](https://ec.europa.eu/eurostat/web/gisco/overview)

* [NUTS downolad page](https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts)

* [TERYT download page](http://eteryt.stat.gov.pl/eTeryt/rejestr_teryt/udostepnianie_danych/formy_i_zasady_udostepniania/formy_i_zasady_udostepniania.aspx?contrast=default)

* [World Borders Dataset](http://thematicmapping.org/downloads/world_borders.php)

* [QGIS tutorials](https://www.qgistutorials.com/en/docs/)

* [gis.stackexchange.com](https://gis.stackexchange.com/)

## Questions?

![](./red_sky1.jpg)

<!--
Local variables:
 ispell-local-dictionary: "english"
End:
-->
